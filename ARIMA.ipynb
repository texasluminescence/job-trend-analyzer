from google.colab import files
dataset = files.upload()

import pandas as pd
import numpy as np
import io

df = pd.read_csv(io.BytesIO(dataset['DataScience_salaries_2024.csv']))
# save only required columns (work year and USD salary)
df.drop(["experience_level", "employment_type", "job_title", "salary", "salary_currency", "employee_residence", "remote_ratio", "company_location", "company_size"], axis=1, inplace=True)
df.dropna()
print(df)

num_jobs = df['work_year'].value_counts()
print(num_jobs)
# unevenly distributed jobs in terms of time -> implement weighted salary aggregation
# underrepresented years (2020-2022) will be weighted heavier; calculate frequency weights for each year
total_jobs = num_jobs.sum()
weights = total_jobs / (5 * num_jobs)
df["weight"] = df["work_year"].map(weights)
print(df)

# add timestamps to each job with uniform quarterly distribution to better fit an ARIMA model
np.random.seed(12) # random nums are now reproducible
df["month"] = np.random.choice(["-01", "-02", "-03", "-04", "-05", "-06", "-07", "-08", "-09", "-10", "-11", "-12"], size=len(df))
# YYYY-Q timestamp format
df["job_date"] = df["work_year"].astype(str) + "-" + df["month"]
df["job_date"] = pd.to_datetime(df["work_year"].astype(str) + df["month"])
# sort over time
df = df.sort_values(by="job_date")
df.drop("weight", "work_year")
print(df)
